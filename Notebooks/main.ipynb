{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d791f33-85d8-4240-a3f8-b06301dbf417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05fc5f-83cf-4c3b-b5eb-a1f3f1af3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:49:05 | INFO     | root | ================================================================================\n",
      "16:49:05 | INFO     | root | Logging initialized | Level: INFO | File: logs\\pipeline_20260122.log\n",
      "16:49:05 | INFO     | root | ================================================================================\n",
      "16:49:05 | INFO     | __main__ | Initializing YouTube Pipeline...\n",
      "16:49:05 | INFO     | src.models.source_tracker | Loaded 1 source(s) from youtube_sources.csv\n",
      "Starting YouTube Pipeline...\n",
      "============================================================\n",
      "16:49:06 | INFO     | __main__ | \n",
      "================================================================================\n",
      "16:49:06 | INFO     | __main__ | YOUTUBE PIPELINE STARTED\n",
      "16:49:06 | INFO     | __main__ | ================================================================================\n",
      "16:49:06 | INFO     | __main__ | \n",
      "Total sources to process: 1\n",
      "16:49:06 | INFO     | __main__ | \n",
      "################################################################################\n",
      "16:49:06 | INFO     | __main__ | SOURCE 1/1: LinusTechTips\n",
      "16:49:06 | INFO     | __main__ | ################################################################################\n",
      "16:49:06 | INFO     | src.models.yt_channel_resolver | Resolving channel ID for: https://www.youtube.com/@LinusTechTips\n",
      "16:49:09 | INFO     | src.models.youtube_finder | Using last_checked time: 2026-01-01 00:00:00+00:00\n",
      "16:49:09 | INFO     | src.models.youtube_finder | Fetching RSS feed from: https://www.youtube.com/feeds/videos.xml?channel_id=UCXuqSBlHAE6Xw-yeJA0Tunw\n",
      "16:49:10 | INFO     | src.models.youtube_finder | Channel: Linus Tech Tips\n",
      "16:49:10 | INFO     | src.models.youtube_finder | Total entries in RSS feed: 15\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'I Bought the Tech House and it Has a PROBLEM…'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-21 18:05:10+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: sPU6wVz2iE8\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'LG's NEWEST SoundSuite Lineup is nuts'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-20 22:33:03+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: KeLmi62DmjU\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'Don’t Wait for the Steam Controller 2!'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-19 18:09:29+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: eNb55ZwnCRc\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'I’m Done Being Mad about Tech'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-17 17:23:47+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: wZZf6LM3wAU\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'My TV Demo Failed - WAN Show January 16, 2026'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-16 23:03:37+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: wcI-lqMwNSY\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'The TRUTH about How LTT SPENDS Money'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-15 18:30:09+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: omCWNO7Jbnw\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'I’m Looking for the WORST Setup on the Internet…'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-14 18:18:20+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: qklfd81ceeY\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'I Joined Robot Fight Club'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-12 18:43:55+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: VJqMPFNP4to\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'The most powerful compact PC'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-12 01:23:02+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: -nTm_xZY3o4\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'CES was frickin weird, guys'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-10 21:29:47+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: Px117so_Mww\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'This Country Gave YouTube The Smack Down - WAN Show January 9, 2026'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-10 04:55:47+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: QFtCeGjKOxU\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'Robot Battery Bank?'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-09 18:48:00+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: Mo-0nz6Mlig\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'He Restored an Old Apple Mouse'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-08 16:42:04+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: iBt1TLBFTRs\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'NVIDIA DLSS 4.5 Explained in 69 seconds'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-07 20:14:03+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: pdM3d0DRY8U\n",
      "16:49:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'Intel’s Best Product in YEARS - Panther Lake Announcement'\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-07 18:11:10+00:00\n",
      "16:49:10 | INFO     | src.models.youtube_finder |     Video ID: bG68OBQ3x9Y\n",
      "16:49:10 | INFO     | src.models.youtube_finder | \\n────────────────────────────────────────────────────────────\n",
      "16:49:10 | INFO     | src.models.youtube_finder | SUMMARY: Found 15 new video(s) after cutoff time\n",
      "16:49:10 | INFO     | src.models.youtube_finder | ────────────────────────────────────────────────────────────\n",
      "16:49:10 | INFO     | __main__ | Processing video: I Bought the Tech House and it Has a PROBLEM…\n",
      "16:49:10 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: sPU6wVz2iE8\n",
      "16:49:10 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: sPU6wVz2iE8 (languages: ['en'])\n",
      "16:49:13 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (18125 characters)\n",
      "16:49:13 | INFO     | src.models.transcript_fetcher |   Raw transcript: 18125 characters\n",
      "16:49:13 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 18125 characters\n",
      "16:49:13 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:49:13 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:49:13 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:49:13 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:49:13 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:49:13 | INFO     | src.models.llm_writer |     Input length: 18125 chars\n",
      "16:49:16 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:49:16 | INFO     | src.models.llm_writer |   ✓ Summary generated: 880 chars\n",
      "16:49:16 | INFO     | src.models.llm_writer |     Tokens used: 4559\n",
      "16:49:16 | INFO     | __main__ | Summary generated for: I Bought the Tech House and it Has a PROBLEM…\n",
      "16:49:16 | INFO     | __main__ | Processing video: LG's NEWEST SoundSuite Lineup is nuts\n",
      "16:49:16 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: KeLmi62DmjU\n",
      "16:49:16 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: KeLmi62DmjU (languages: ['en'])\n",
      "16:49:18 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (1189 characters)\n",
      "16:49:18 | INFO     | src.models.transcript_fetcher |   Raw transcript: 1189 characters\n",
      "16:49:18 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 1189 characters\n",
      "16:49:18 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:49:18 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:49:18 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:49:18 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:49:18 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:49:18 | INFO     | src.models.llm_writer |     Input length: 1189 chars\n",
      "16:49:19 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:49:19 | INFO     | src.models.llm_writer |   ✓ Summary generated: 428 chars\n",
      "16:49:19 | INFO     | src.models.llm_writer |     Tokens used: 423\n",
      "16:49:19 | INFO     | __main__ | Summary generated for: LG's NEWEST SoundSuite Lineup is nuts\n",
      "16:49:19 | INFO     | __main__ | Processing video: Don’t Wait for the Steam Controller 2!\n",
      "16:49:19 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: eNb55ZwnCRc\n",
      "16:49:19 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: eNb55ZwnCRc (languages: ['en'])\n",
      "16:49:22 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (26341 characters)\n",
      "16:49:22 | INFO     | src.models.transcript_fetcher |   Raw transcript: 26341 characters\n",
      "16:49:22 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 26341 characters\n",
      "16:49:22 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:49:22 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:49:22 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:49:22 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:49:22 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:49:22 | INFO     | src.models.llm_writer |     Input length: 26341 chars\n",
      "16:49:24 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:49:24 | INFO     | src.models.llm_writer |   ✓ Summary generated: 1427 chars\n",
      "16:49:24 | INFO     | src.models.llm_writer |     Tokens used: 6700\n",
      "16:49:24 | INFO     | __main__ | Summary generated for: Don’t Wait for the Steam Controller 2!\n",
      "16:49:24 | INFO     | __main__ | Processing video: I’m Done Being Mad about Tech\n",
      "16:49:24 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: wZZf6LM3wAU\n",
      "16:49:24 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: wZZf6LM3wAU (languages: ['en'])\n",
      "16:49:27 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (14803 characters)\n",
      "16:49:27 | INFO     | src.models.transcript_fetcher |   Raw transcript: 14803 characters\n",
      "16:49:27 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 14803 characters\n",
      "16:49:27 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:49:27 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:49:27 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:49:27 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:49:27 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:49:27 | INFO     | src.models.llm_writer |     Input length: 14803 chars\n",
      "16:49:27 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "16:49:27 | INFO     | openai._base_client | Retrying request to /chat/completions in 3.000000 seconds\n",
      "16:49:31 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:49:31 | INFO     | src.models.llm_writer |   ✓ Summary generated: 1057 chars\n",
      "16:49:31 | INFO     | src.models.llm_writer |     Tokens used: 3519\n",
      "16:49:31 | INFO     | __main__ | Summary generated for: I’m Done Being Mad about Tech\n",
      "16:49:31 | INFO     | __main__ | Processing video: My TV Demo Failed - WAN Show January 16, 2026\n",
      "16:49:31 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: wcI-lqMwNSY\n",
      "16:49:31 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: wcI-lqMwNSY (languages: ['en'])\n",
      "16:49:34 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (171678 characters)\n",
      "16:49:34 | INFO     | src.models.transcript_fetcher |   Raw transcript: 171678 characters\n",
      "16:49:34 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 171678 characters\n",
      "16:49:34 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:49:34 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:49:34 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:49:34 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:49:34 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:49:34 | INFO     | src.models.llm_writer |     Input length: 171678 chars\n",
      "16:49:35 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 413 Payload Too Large\"\n",
      "16:49:35 | ERROR    | src.models.llm_writer |   ✗ Groq API error: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jq197zw3fwctspwse9b7494a` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 41170, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "16:49:35 | INFO     | __main__ | Summary generated for: My TV Demo Failed - WAN Show January 16, 2026\n",
      "16:49:35 | INFO     | __main__ | Processing video: The TRUTH about How LTT SPENDS Money\n",
      "16:49:35 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: omCWNO7Jbnw\n",
      "16:49:35 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: omCWNO7Jbnw (languages: ['en'])\n",
      "16:49:37 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (22359 characters)\n",
      "16:49:37 | INFO     | src.models.transcript_fetcher |   Raw transcript: 22359 characters\n",
      "16:49:37 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 22359 characters\n",
      "16:49:37 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:49:37 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:49:37 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:49:37 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:49:37 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:49:37 | INFO     | src.models.llm_writer |     Input length: 22359 chars\n",
      "16:49:37 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "16:49:37 | INFO     | openai._base_client | Retrying request to /chat/completions in 18.000000 seconds\n",
      "16:49:58 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:49:58 | INFO     | src.models.llm_writer |   ✓ Summary generated: 1846 chars\n",
      "16:49:58 | INFO     | src.models.llm_writer |     Tokens used: 5243\n",
      "16:49:58 | INFO     | __main__ | Summary generated for: The TRUTH about How LTT SPENDS Money\n",
      "16:49:58 | INFO     | __main__ | Processing video: I’m Looking for the WORST Setup on the Internet…\n",
      "16:49:58 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: qklfd81ceeY\n",
      "16:49:58 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: qklfd81ceeY (languages: ['en'])\n",
      "16:50:00 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (16282 characters)\n",
      "16:50:00 | INFO     | src.models.transcript_fetcher |   Raw transcript: 16282 characters\n",
      "16:50:00 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 16282 characters\n",
      "16:50:00 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:00 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:00 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:00 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:00 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:00 | INFO     | src.models.llm_writer |     Input length: 16282 chars\n",
      "16:50:00 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "16:50:00 | INFO     | openai._base_client | Retrying request to /chat/completions in 19.000000 seconds\n",
      "16:50:20 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:20 | INFO     | src.models.llm_writer |   ✓ Summary generated: 683 chars\n",
      "16:50:20 | INFO     | src.models.llm_writer |     Tokens used: 4468\n",
      "16:50:20 | INFO     | __main__ | Summary generated for: I’m Looking for the WORST Setup on the Internet…\n",
      "16:50:20 | INFO     | __main__ | Processing video: I Joined Robot Fight Club\n",
      "16:50:20 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: VJqMPFNP4to\n",
      "16:50:20 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: VJqMPFNP4to (languages: ['en'])\n",
      "16:50:23 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (11108 characters)\n",
      "16:50:23 | INFO     | src.models.transcript_fetcher |   Raw transcript: 11108 characters\n",
      "16:50:23 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 11108 characters\n",
      "16:50:23 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:23 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:23 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:23 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:23 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:23 | INFO     | src.models.llm_writer |     Input length: 11108 chars\n",
      "16:50:23 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "16:50:23 | INFO     | openai._base_client | Retrying request to /chat/completions in 11.000000 seconds\n",
      "16:50:35 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:35 | INFO     | src.models.llm_writer |   ✓ Summary generated: 807 chars\n",
      "16:50:35 | INFO     | src.models.llm_writer |     Tokens used: 3023\n",
      "16:50:35 | INFO     | __main__ | Summary generated for: I Joined Robot Fight Club\n",
      "16:50:35 | INFO     | __main__ | Processing video: The most powerful compact PC\n",
      "16:50:35 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: -nTm_xZY3o4\n",
      "16:50:35 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: -nTm_xZY3o4 (languages: ['en'])\n",
      "16:50:37 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (619 characters)\n",
      "16:50:37 | INFO     | src.models.transcript_fetcher |   Raw transcript: 619 characters\n",
      "16:50:37 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 619 characters\n",
      "16:50:37 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:37 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:37 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:37 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:37 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:37 | INFO     | src.models.llm_writer |     Input length: 619 chars\n",
      "16:50:38 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:38 | INFO     | src.models.llm_writer |   ✓ Summary generated: 167 chars\n",
      "16:50:38 | INFO     | src.models.llm_writer |     Tokens used: 245\n",
      "16:50:38 | INFO     | __main__ | Summary generated for: The most powerful compact PC\n",
      "16:50:38 | INFO     | __main__ | Processing video: CES was frickin weird, guys\n",
      "16:50:38 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: Px117so_Mww\n",
      "16:50:38 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: Px117so_Mww (languages: ['en'])\n",
      "16:50:41 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (6619 characters)\n",
      "16:50:41 | INFO     | src.models.transcript_fetcher |   Raw transcript: 6619 characters\n",
      "16:50:41 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 6619 characters\n",
      "16:50:41 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:41 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:41 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:41 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:41 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:41 | INFO     | src.models.llm_writer |     Input length: 6619 chars\n",
      "16:50:41 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "16:50:41 | INFO     | openai._base_client | Retrying request to /chat/completions in 4.000000 seconds\n",
      "16:50:45 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:45 | INFO     | src.models.llm_writer |   ✓ Summary generated: 810 chars\n",
      "16:50:45 | INFO     | src.models.llm_writer |     Tokens used: 1786\n",
      "16:50:45 | INFO     | __main__ | Summary generated for: CES was frickin weird, guys\n",
      "16:50:45 | INFO     | __main__ | Processing video: This Country Gave YouTube The Smack Down - WAN Show January 9, 2026\n",
      "16:50:45 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: QFtCeGjKOxU\n",
      "16:50:45 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: QFtCeGjKOxU (languages: ['en'])\n",
      "16:50:48 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (198671 characters)\n",
      "16:50:48 | INFO     | src.models.transcript_fetcher |   Raw transcript: 198671 characters\n",
      "16:50:48 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 198671 characters\n",
      "16:50:48 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:48 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:48 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:48 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:48 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:48 | INFO     | src.models.llm_writer |     Input length: 198671 chars\n",
      "16:50:48 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 413 Payload Too Large\"\n",
      "16:50:48 | ERROR    | src.models.llm_writer |   ✗ Groq API error: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jq197zw3fwctspwse9b7494a` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 48266, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "16:50:48 | INFO     | __main__ | Summary generated for: This Country Gave YouTube The Smack Down - WAN Show January 9, 2026\n",
      "16:50:48 | INFO     | __main__ | Processing video: Robot Battery Bank?\n",
      "16:50:49 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: Mo-0nz6Mlig\n",
      "16:50:49 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: Mo-0nz6Mlig (languages: ['en'])\n",
      "16:50:51 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (758 characters)\n",
      "16:50:51 | INFO     | src.models.transcript_fetcher |   Raw transcript: 758 characters\n",
      "16:50:51 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 758 characters\n",
      "16:50:51 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:51 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:51 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:51 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:51 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:51 | INFO     | src.models.llm_writer |     Input length: 758 chars\n",
      "16:50:51 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:51 | INFO     | src.models.llm_writer |   ✓ Summary generated: 282 chars\n",
      "16:50:51 | INFO     | src.models.llm_writer |     Tokens used: 299\n",
      "16:50:51 | INFO     | __main__ | Summary generated for: Robot Battery Bank?\n",
      "16:50:51 | INFO     | __main__ | Processing video: He Restored an Old Apple Mouse\n",
      "16:50:51 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: iBt1TLBFTRs\n",
      "16:50:51 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: iBt1TLBFTRs (languages: ['en'])\n",
      "16:50:54 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (517 characters)\n",
      "16:50:54 | INFO     | src.models.transcript_fetcher |   Raw transcript: 517 characters\n",
      "16:50:54 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 517 characters\n",
      "16:50:54 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:54 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:54 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:54 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:54 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:54 | INFO     | src.models.llm_writer |     Input length: 517 chars\n",
      "16:50:54 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:54 | INFO     | src.models.llm_writer |   ✓ Summary generated: 212 chars\n",
      "16:50:54 | INFO     | src.models.llm_writer |     Tokens used: 214\n",
      "16:50:54 | INFO     | __main__ | Summary generated for: He Restored an Old Apple Mouse\n",
      "16:50:54 | INFO     | __main__ | Processing video: NVIDIA DLSS 4.5 Explained in 69 seconds\n",
      "16:50:54 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: pdM3d0DRY8U\n",
      "16:50:54 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: pdM3d0DRY8U (languages: ['en'])\n",
      "16:50:57 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (1333 characters)\n",
      "16:50:57 | INFO     | src.models.transcript_fetcher |   Raw transcript: 1333 characters\n",
      "16:50:57 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 1333 characters\n",
      "16:50:57 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:50:57 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:50:57 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:50:57 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:50:57 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:50:57 | INFO     | src.models.llm_writer |     Input length: 1333 chars\n",
      "16:50:58 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:50:58 | INFO     | src.models.llm_writer |   ✓ Summary generated: 521 chars\n",
      "16:50:58 | INFO     | src.models.llm_writer |     Tokens used: 464\n",
      "16:50:58 | INFO     | __main__ | Summary generated for: NVIDIA DLSS 4.5 Explained in 69 seconds\n",
      "16:50:58 | INFO     | __main__ | Processing video: Intel’s Best Product in YEARS - Panther Lake Announcement\n",
      "16:50:58 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: bG68OBQ3x9Y\n",
      "16:50:58 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: bG68OBQ3x9Y (languages: ['en'])\n",
      "16:51:00 | INFO     | src.models.transcript_fetcher | Transcript fetched successfully (9146 characters)\n",
      "16:51:00 | INFO     | src.models.transcript_fetcher |   Raw transcript: 9146 characters\n",
      "16:51:00 | INFO     | src.models.transcript_fetcher |   ✓ Cleaned transcript: 9146 characters\n",
      "16:51:00 | INFO     | __main__ | Transcript fetched, generating summary...\n",
      "16:51:00 | INFO     | src.models.llm_writer |   Calling Groq API...\n",
      "16:51:00 | INFO     | src.models.llm_writer |     Model: llama-3.3-70b-versatile\n",
      "16:51:00 | INFO     | src.models.llm_writer |     Temperature: 0.7\n",
      "16:51:00 | INFO     | src.models.llm_writer |     Max tokens: 1024\n",
      "16:51:00 | INFO     | src.models.llm_writer |     Input length: 9146 chars\n",
      "16:51:01 | INFO     | httpx | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "16:51:01 | INFO     | src.models.llm_writer |   ✓ Summary generated: 896 chars\n",
      "16:51:01 | INFO     | src.models.llm_writer |     Tokens used: 2304\n",
      "16:51:01 | INFO     | __main__ | Summary generated for: Intel’s Best Product in YEARS - Panther Lake Announcement\n",
      "16:51:01 | INFO     | __main__ | Processed 15 items from channel\n",
      "16:51:01 | INFO     | __main__ | \n",
      "================================================================================\n",
      "16:51:01 | INFO     | __main__ | PIPELINE COMPLETE\n",
      "16:51:01 | INFO     | __main__ | Total items generated: 15\n",
      "16:51:01 | INFO     | __main__ | \n",
      "Generated items:\n",
      "16:51:01 | INFO     | __main__ |   1. I Bought the Tech House and it Has a PROBLEM…\n",
      "16:51:01 | INFO     | __main__ |   2. LG's NEWEST SoundSuite Lineup is nuts\n",
      "16:51:01 | INFO     | __main__ |   3. Don’t Wait for the Steam Controller 2!\n",
      "16:51:01 | INFO     | __main__ |   4. I’m Done Being Mad about Tech\n",
      "16:51:01 | INFO     | __main__ |   5. My TV Demo Failed - WAN Show January 16, 2026\n",
      "16:51:01 | INFO     | __main__ |   6. The TRUTH about How LTT SPENDS Money\n",
      "16:51:01 | INFO     | __main__ |   7. I’m Looking for the WORST Setup on the Internet…\n",
      "16:51:01 | INFO     | __main__ |   8. I Joined Robot Fight Club\n",
      "16:51:01 | INFO     | __main__ |   9. The most powerful compact PC\n",
      "16:51:01 | INFO     | __main__ |   10. CES was frickin weird, guys\n",
      "16:51:01 | INFO     | __main__ |   11. This Country Gave YouTube The Smack Down - WAN Show January 9, 2026\n",
      "16:51:01 | INFO     | __main__ |   12. Robot Battery Bank?\n",
      "16:51:01 | INFO     | __main__ |   13. He Restored an Old Apple Mouse\n",
      "16:51:01 | INFO     | __main__ |   14. NVIDIA DLSS 4.5 Explained in 69 seconds\n",
      "16:51:01 | INFO     | __main__ |   15. Intel’s Best Product in YEARS - Panther Lake Announcement\n",
      "16:51:01 | INFO     | __main__ | ================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    111\u001b[39m result = pipeline.run()\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDaily Digest Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal Items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result[\u001b[33m'\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from typing import Dict, Any, List\n",
    "from src.models.source_tracker import SourceTracker\n",
    "from src.models.youtube_finder import YouTubeVideoFinder\n",
    "from src.models.transcript_fetcher import TranscriptFetcher\n",
    "from src.models.yt_channel_resolver import YouTubeChannelResolver\n",
    "from src.models.llm_writer import LLMWriter\n",
    "from src.utils.json_builder import JSONBuilder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class YouTubePipeline:\n",
    "    def __init__(self, config_path: str = \"config.yaml\"):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "\n",
    "        self.source_tracker = SourceTracker(self.config[\"pipeline\"][\"sources_file\"])\n",
    "        self.video_finder = YouTubeVideoFinder()\n",
    "        self.channel_resolver = YouTubeChannelResolver()\n",
    "        self.transcript_fetcher = TranscriptFetcher()\n",
    "        self.llm_writer = LLMWriter(config_path=config_path)\n",
    "\n",
    "    def process_single_channel(self, source: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Handles the logic for a single source entry.\"\"\"\n",
    "        items = []\n",
    "        channel_url = source.get(\"url\")\n",
    "        last_checked = source.get(\"last_checked\")\n",
    "\n",
    "        try:\n",
    "            channel_id = self.channel_resolver.get_channel_id(channel_url)\n",
    "            videos = self.video_finder.find_new_videos(\n",
    "                channel_id, last_checked=last_checked\n",
    "            )\n",
    "\n",
    "            for video in videos:\n",
    "                logger.info(f\"Processing video: {video['title']}\")\n",
    "                transcript = self.transcript_fetcher.fetch_and_clean(\n",
    "                    video_id=video[\"video_id\"], title=video[\"title\"]\n",
    "                )\n",
    "                if transcript:\n",
    "                    logger.info(f\"Transcript fetched, generating summary...\")\n",
    "                    res = self.llm_writer.process_content(\n",
    "                        video[\"title\"], transcript[\"clean_text\"]\n",
    "                    )\n",
    "                    items.append(\n",
    "                        JSONBuilder.build_item(\n",
    "                            item_type=\"youtube\",\n",
    "                            title=res[\"title\"],\n",
    "                            summary=res[\"summary\"],\n",
    "                            link=video[\"link\"],\n",
    "                        )\n",
    "                    )\n",
    "                    logger.info(f\"Summary generated for: {video['title']}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No transcript available for: {video['title']}\")\n",
    "\n",
    "            logger.info(f\"Processed {len(items)} items from channel\")\n",
    "            return items\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing channel {channel_url}: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def run(self) -> Dict[str, Any]:\n",
    "        \"\"\"Main execution loop for all sources.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"YOUTUBE PIPELINE STARTED\")\n",
    "        logger.info(\"=\" * 80)\n",
    "\n",
    "        all_items = []\n",
    "        sources = self.source_tracker.get_sources()\n",
    "        logger.info(f\"\\nTotal sources to process: {len(sources)}\")\n",
    "\n",
    "        for idx, source in enumerate(sources, 1):\n",
    "            logger.info(f\"\\n{'#'*80}\")\n",
    "            logger.info(f\"SOURCE {idx}/{len(sources)}: {source.get('name', 'Unknown')}\")\n",
    "            logger.info(f\"{'#'*80}\")\n",
    "            channel_items = self.process_single_channel(source)\n",
    "            all_items.extend(channel_items)\n",
    "\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"PIPELINE COMPLETE\")\n",
    "        logger.info(f\"Total items generated: {len(all_items)}\")\n",
    "        if all_items:\n",
    "            logger.info(\"\\nGenerated items:\")\n",
    "            for idx, item in enumerate(all_items, 1):\n",
    "                logger.info(f\"  {idx}. {item['title']}\")\n",
    "        logger.info(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    from src.utils.logger import setup_logging\n",
    "\n",
    "    # Setup industry-grade logging\n",
    "    setup_logging(\n",
    "        log_level=\"INFO\",  # Change to DEBUG for more detailed logs\n",
    "        log_dir=\"logs\",\n",
    "        console=True,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Initializing YouTube Pipeline...\")\n",
    "\n",
    "    # Initialize the pipeline with actual config\n",
    "    pipeline = YouTubePipeline(config_path=\"src/config/config.yaml\")\n",
    "    print(\"Starting YouTube Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run the pipeline with real YouTube sources\n",
    "    result = pipeline.run()\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nDaily Digest Generated: {result['date']}\")\n",
    "    print(f\"Total Items: {len(result['items'])}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for idx, item in enumerate(result[\"items\"], 1):\n",
    "        print(f\"\\n[{idx}] {item['title']}\")\n",
    "        print(f\"Type: {item['type']}\")\n",
    "        print(f\"Link: {item['link']}\")\n",
    "        print(f\"Summary:\\n{item['summary']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # Save to file\n",
    "    output_file = \"daily_digest.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8aff69-b9e0-44c1-896c-df52b676492c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mitems\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[32m1\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mType: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(result[\"items\"], 1):\n",
    "    print(f\"\\n[{idx}] {item['title']}\")\n",
    "    print(f\"Type: {item['type']}\")\n",
    "    print(f\"Link: {item['link']}\")\n",
    "    print(f\"Summary:\\n{item['summary']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297b930-76cf-4a2a-97a6-37a338fa6615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:56:04 | INFO     | root | ================================================================================\n",
      "16:56:04 | INFO     | root | Logging initialized | Level: INFO | File: logs\\pipeline_20260122.log\n",
      "16:56:04 | INFO     | root | ================================================================================\n",
      "16:56:04 | INFO     | __main__ | Initializing YouTube Pipeline...\n",
      "16:56:04 | INFO     | src.models.source_tracker | Loaded 1 source(s) from youtube_sources.csv\n",
      "Starting YouTube Pipeline...\n",
      "============================================================\n",
      "16:56:06 | INFO     | __main__ | \n",
      "================================================================================\n",
      "16:56:06 | INFO     | __main__ | YOUTUBE PIPELINE STARTED\n",
      "16:56:06 | INFO     | __main__ | ================================================================================\n",
      "16:56:06 | INFO     | __main__ | \n",
      "Total sources to process: 1\n",
      "16:56:06 | INFO     | __main__ | \n",
      "################################################################################\n",
      "16:56:06 | INFO     | __main__ | SOURCE 1/1: LinusTechTips\n",
      "16:56:06 | INFO     | __main__ | ################################################################################\n",
      "16:56:06 | INFO     | src.models.yt_channel_resolver | Resolving channel ID for: https://www.youtube.com/@LinusTechTips\n",
      "16:56:08 | INFO     | src.models.youtube_finder | Using relative time: 48 hours ago = 2026-01-20 11:26:08.479257+00:00\n",
      "16:56:08 | INFO     | src.models.youtube_finder | Fetching RSS feed from: https://www.youtube.com/feeds/videos.xml?channel_id=UCXuqSBlHAE6Xw-yeJA0Tunw\n",
      "16:56:10 | INFO     | src.models.youtube_finder | Channel: Linus Tech Tips\n",
      "16:56:10 | INFO     | src.models.youtube_finder | Total entries in RSS feed: 15\n",
      "16:56:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'I Bought the Tech House and it Has a PROBLEM…'\n",
      "16:56:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-21 18:05:10+00:00\n",
      "16:56:10 | INFO     | src.models.youtube_finder |     Video ID: sPU6wVz2iE8\n",
      "16:56:10 | INFO     | src.models.youtube_finder |   ✓ New video found: 'LG's NEWEST SoundSuite Lineup is nuts'\n",
      "16:56:10 | INFO     | src.models.youtube_finder |     Published: 2026-01-20 22:33:03+00:00\n",
      "16:56:10 | INFO     | src.models.youtube_finder |     Video ID: KeLmi62DmjU\n",
      "16:56:10 | INFO     | src.models.youtube_finder | \\n────────────────────────────────────────────────────────────\n",
      "16:56:10 | INFO     | src.models.youtube_finder | SUMMARY: Found 2 new video(s) after cutoff time\n",
      "16:56:10 | INFO     | src.models.youtube_finder | ────────────────────────────────────────────────────────────\n",
      "16:56:10 | INFO     | __main__ | Processing video: I Bought the Tech House and it Has a PROBLEM…\n",
      "16:56:10 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: sPU6wVz2iE8\n",
      "16:56:10 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: sPU6wVz2iE8 (languages: ['en'])\n",
      "16:56:13 | ERROR    | src.models.transcript_fetcher | Could not fetch transcript for video sPU6wVz2iE8: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=sPU6wVz2iE8! This is most likely caused by:\n",
      "\n",
      "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
      "- You have done too many requests and your IP has been blocked by YouTube\n",
      "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
      "\n",
      "Ways to work around this are explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
      "\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "16:56:13 | WARNING  | src.models.transcript_fetcher |   ✗ No transcript available\n",
      "16:56:13 | WARNING  | __main__ | No transcript available for: I Bought the Tech House and it Has a PROBLEM…\n",
      "16:56:13 | INFO     | __main__ | Processing video: LG's NEWEST SoundSuite Lineup is nuts\n",
      "16:56:13 | INFO     | src.models.transcript_fetcher |   Fetching transcript for video ID: KeLmi62DmjU\n",
      "16:56:13 | INFO     | src.models.transcript_fetcher | Fetching transcript for video: KeLmi62DmjU (languages: ['en'])\n",
      "16:56:17 | ERROR    | src.models.transcript_fetcher | Could not fetch transcript for video KeLmi62DmjU: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=KeLmi62DmjU! This is most likely caused by:\n",
      "\n",
      "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
      "- You have done too many requests and your IP has been blocked by YouTube\n",
      "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
      "\n",
      "Ways to work around this are explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
      "\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "16:56:17 | WARNING  | src.models.transcript_fetcher |   ✗ No transcript available\n",
      "16:56:17 | WARNING  | __main__ | No transcript available for: LG's NEWEST SoundSuite Lineup is nuts\n",
      "16:56:17 | INFO     | __main__ | Processed 0 items from channel\n",
      "16:56:17 | INFO     | __main__ | \n",
      "================================================================================\n",
      "16:56:17 | INFO     | __main__ | PIPELINE COMPLETE\n",
      "16:56:17 | INFO     | __main__ | Total items generated: 0\n",
      "16:56:17 | INFO     | __main__ | ================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m result = pipeline.run()\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDaily Digest Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal Items: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result[\u001b[33m'\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from typing import Dict, Any, List\n",
    "from src.models.source_tracker import SourceTracker\n",
    "from src.models.youtube_finder import YouTubeVideoFinder\n",
    "from src.models.transcript_fetcher import TranscriptFetcher\n",
    "from src.models.yt_channel_resolver import YouTubeChannelResolver\n",
    "from src.models.llm_writer import LLMWriter\n",
    "from src.utils.json_builder import JSONBuilder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class YouTubePipeline:\n",
    "    def __init__(self, config_path: str = \"config.yaml\"):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "\n",
    "        self.source_tracker = SourceTracker(self.config[\"pipeline\"][\"sources_file\"])\n",
    "        self.video_finder = YouTubeVideoFinder()\n",
    "        self.channel_resolver = YouTubeChannelResolver()\n",
    "        self.transcript_fetcher = TranscriptFetcher()\n",
    "        self.llm_writer = LLMWriter(config_path=config_path)\n",
    "\n",
    "    def process_single_channel(self, source: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Handles the logic for a single source entry.\"\"\"\n",
    "        items = []\n",
    "        channel_url = source.get(\"url\")\n",
    "        last_checked = source.get(\"last_checked\")\n",
    "\n",
    "        try:\n",
    "            channel_id = self.channel_resolver.get_channel_id(channel_url)\n",
    "            # Use 48-hour window instead of last_checked to avoid stale dates from CSV\n",
    "            videos = self.video_finder.find_new_videos(channel_id, hours=48)\n",
    "\n",
    "            for video in videos:\n",
    "                logger.info(f\"Processing video: {video['title']}\")\n",
    "                transcript = self.transcript_fetcher.fetch_and_clean(\n",
    "                    video_id=video[\"video_id\"], title=video[\"title\"]\n",
    "                )\n",
    "                if transcript:\n",
    "                    logger.info(f\"Transcript fetched, generating summary...\")\n",
    "                    res = self.llm_writer.process_content(\n",
    "                        video[\"title\"], transcript[\"clean_text\"]\n",
    "                    )\n",
    "                    items.append(\n",
    "                        JSONBuilder.build_item(\n",
    "                            item_type=\"youtube\",\n",
    "                            title=res[\"title\"],\n",
    "                            summary=res[\"summary\"],\n",
    "                            link=video[\"link\"],\n",
    "                        )\n",
    "                    )\n",
    "                    logger.info(f\"Summary generated for: {video['title']}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No transcript available for: {video['title']}\")\n",
    "\n",
    "            logger.info(f\"Processed {len(items)} items from channel\")\n",
    "            return items\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing channel {channel_url}: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def run(self) -> Dict[str, Any]:\n",
    "        \"\"\"Main execution loop for all sources.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"YOUTUBE PIPELINE STARTED\")\n",
    "        logger.info(\"=\" * 80)\n",
    "\n",
    "        all_items = []\n",
    "        sources = self.source_tracker.get_sources()\n",
    "        logger.info(f\"\\nTotal sources to process: {len(sources)}\")\n",
    "\n",
    "        for idx, source in enumerate(sources, 1):\n",
    "            logger.info(f\"\\n{'#'*80}\")\n",
    "            logger.info(f\"SOURCE {idx}/{len(sources)}: {source.get('name', 'Unknown')}\")\n",
    "            logger.info(f\"{'#'*80}\")\n",
    "            channel_items = self.process_single_channel(source)\n",
    "            all_items.extend(channel_items)\n",
    "\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"PIPELINE COMPLETE\")\n",
    "        logger.info(f\"Total items generated: {len(all_items)}\")\n",
    "        if all_items:\n",
    "            logger.info(\"\\nGenerated items:\")\n",
    "            for idx, item in enumerate(all_items, 1):\n",
    "                logger.info(f\"  {idx}. {item['title']}\")\n",
    "        logger.info(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    from src.utils.logger import setup_logging\n",
    "\n",
    "    # Setup industry-grade logging\n",
    "    setup_logging(\n",
    "        log_level=\"INFO\",  # Change to DEBUG for more detailed logs\n",
    "        log_dir=\"logs\",\n",
    "        console=True,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Initializing YouTube Pipeline...\")\n",
    "\n",
    "    # Initialize the pipeline with actual config\n",
    "    pipeline = YouTubePipeline(config_path=\"src/config/config.yaml\")\n",
    "    print(\"Starting YouTube Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run the pipeline with real YouTube sources\n",
    "    result = pipeline.run()\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nDaily Digest Generated: {result['date']}\")\n",
    "    print(f\"Total Items: {len(result['items'])}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for idx, item in enumerate(result[\"items\"], 1):\n",
    "        print(f\"\\n[{idx}] {item['title']}\")\n",
    "        print(f\"Type: {item['type']}\")\n",
    "        print(f\"Link: {item['link']}\")\n",
    "        print(f\"Summary:\\n{item['summary']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # Save to file\n",
    "    output_file = \"daily_digest.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
